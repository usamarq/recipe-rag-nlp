{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3683f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np # Added numpy for percentile calculation\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Setup\n",
    "# ---------------------------------------------\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "# Assuming 'df' is your DataFrame loaded and columns cleaned\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Define columns to explore\n",
    "# ---------------------------------------------\n",
    "nutritional_cols = [\n",
    "    'calories_cal',\n",
    "    'protein_g',\n",
    "    'totalfat_g',\n",
    "    'saturatedfat_g',\n",
    "    'cholesterol_mg',\n",
    "    'sodium_mg',\n",
    "    'totalcarbohydrate_g',\n",
    "    'dietaryfiber_g',\n",
    "    'sugars_g',\n",
    "    'duration',\n",
    "    'ingredients_sizes',\n",
    "    'who_score',\n",
    "    'fsa_score',\n",
    "    'nutri_score'\n",
    "\n",
    "]\n",
    "\n",
    "# Nicely formatted titles for plots\n",
    "titles = {\n",
    "    'calories_cal': 'Calories Distribution (99th Percentile)',\n",
    "    'protein_g': 'Protein Distribution (99th Percentile)',\n",
    "    'totalfat_g': 'Total Fat Distribution (99th Percentile)',\n",
    "    'saturatedfat_g': 'Saturated Fat Distribution (99th Percentile)',\n",
    "    'cholesterol_mg': 'Cholesterol Distribution (99th Percentile)',\n",
    "    'sodium_mg': 'Sodium Distribution (99th Percentile)',\n",
    "    'totalcarbohydrate_g': 'Total Carbohydrates Distribution (99th Percentile)',\n",
    "    'dietaryfiber_g': 'Dietary Fiber Distribution (99th Percentile)',\n",
    "    'sugars_g': 'Sugar Distribution (99th Percentile)',\n",
    "    'duration': 'Cooking Duration Distribution (99th Percentile)',\n",
    "    'ingredients_sizes': 'Ingredient Sizes Distribution (99th Percentile)',\n",
    "    'who_score': 'WHO Score Distribution', # Scores usually don't need filtering, but kept consistent for structure\n",
    "    'fsa_score': 'FSA Score Distribution',\n",
    "    'nutri_score': 'Nutritional Score Distribution'\n",
    "}\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Plot all nutritional distributions in a grid (with percentile filtering)\n",
    "# ---------------------------------------------\n",
    "n = len(nutritional_cols)\n",
    "rows = (n + 2) // 3  # roughly 3 per row\n",
    "\n",
    "fig, axes = plt.subplots(rows, 3, figsize=(18, 5 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(nutritional_cols):\n",
    "    if col in df.columns:\n",
    "        # --- Filtering Added Here ---\n",
    "        data_to_plot = df[col].dropna() # Drop NaNs for calculation and plotting\n",
    "        if pd.api.types.is_numeric_dtype(data_to_plot) and not data_to_plot.empty:\n",
    "            q99 = np.percentile(data_to_plot, 99)\n",
    "            # Apply filtering mainly to columns prone to extreme outliers\n",
    "            # Scores might not need it, but apply consistently unless specified otherwise\n",
    "            if q99 > 0: # Avoid filtering if q99 is 0 or negative\n",
    "               filtered_data = data_to_plot[data_to_plot <= q99]\n",
    "            else:\n",
    "               filtered_data = data_to_plot # Don't filter if percentile is non-positive\n",
    "        else:\n",
    "            filtered_data = data_to_plot # Use original data if not numeric or empty after dropna\n",
    "\n",
    "        # Check if filtered_data is empty before plotting\n",
    "        if not filtered_data.empty:\n",
    "             sns.histplot(filtered_data, bins=50, ax=axes[i], color='mediumseagreen', edgecolor='black', kde=True)\n",
    "             axes[i].set_title(titles[col], fontsize=14)\n",
    "             axes[i].set_xlabel(col.replace('_', ' ').title())\n",
    "             # Optionally set xlim to focus the view, especially for filtered columns\n",
    "             if col not in ['who_score', 'fsa_score', 'nutri_score'] and q99 > 0: # Check q99 > 0\n",
    "                  axes[i].set_xlim(0, q99)\n",
    "             axes[i].set_ylabel('Frequency') # Add y-label\n",
    "        else:\n",
    "             axes[i].set_title(f\"{titles[col]}\\n(No data after filtering)\", fontsize=14)\n",
    "             axes[i].set_xlabel(col.replace('_', ' ').title())\n",
    "             axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    else:\n",
    "        axes[i].set_visible(False) # Hide axis if column not found\n",
    "\n",
    "# Hide any empty subplots if the number of columns < grid size\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle('Nutritional Attribute Distributions (Filtered to 99th Percentile)', fontsize=20)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# bm25_search.py (initial unoptimized version)\n",
    "# Task 3 – Simple BM25-based retrieval for recipe search\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from text_preprocessing import preprocess_text\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Load the preprocessed dataset\n",
    "# -------------------------------------------------\n",
    "def load_dataset(path=\"data/hummus_recipes_preprocessed.csv\"):\n",
    "    print(f\"📂 Loading dataset from: {path}\")\n",
    "    df = pd.read_csv(path, low_memory=True)\n",
    "    print(f\"✅ Dataset shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Build or load BM25 index\n",
    "# -------------------------------------------------\n",
    "def build_bm25_index(df, index_path=\"data/bm25_index.pkl\", tokens_path=\"data/tokenized_docs.pkl\"):\n",
    "    \"\"\"\n",
    "    Builds or loads a cached BM25 index and tokenized documents.\n",
    "    \"\"\"\n",
    "    if os.path.exists(index_path) and os.path.exists(tokens_path):\n",
    "        print(f\"📦 Loading cached BM25 index from disk...\")\n",
    "        with open(index_path, \"rb\") as f:\n",
    "            bm25 = pickle.load(f)\n",
    "        tokenized_docs = joblib.load(tokens_path)\n",
    "        print(f\"✅ Loaded BM25 index for {len(tokenized_docs)} recipes.\")\n",
    "        return bm25, tokenized_docs\n",
    "\n",
    "    print(\"⚙️ Building new BM25 index...\")\n",
    "\n",
    "    # Combine relevant processed text columns\n",
    "    text_cols = [\"processed_title\", \"processed_ingredients\", \"processed_tags\", \"processed_directions\"]\n",
    "    available_cols = [c for c in text_cols if c in df.columns]\n",
    "    print(f\"Using columns: {available_cols}\")\n",
    "\n",
    "    # Combine text into one document per recipe\n",
    "    df[\"combined_text\"] = df[available_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "    # Tokenize (already preprocessed)\n",
    "    tokenized_docs = [doc.split() for doc in df[\"combined_text\"]]\n",
    "\n",
    "    # Build BM25\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "    # Save both index and tokenized docs for reuse\n",
    "    with open(index_path, \"wb\") as f:\n",
    "        pickle.dump(bm25, f)\n",
    "    joblib.dump(tokenized_docs, tokens_path)\n",
    "\n",
    "    print(f\"✅ BM25 index built and saved for {len(tokenized_docs)} recipes.\")\n",
    "    return bm25, tokenized_docs\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Preprocess and search\n",
    "# -------------------------------------------------\n",
    "def preprocess_query(query: str):\n",
    "    \"\"\"Apply the same preprocessing as dataset text.\"\"\"\n",
    "    return preprocess_text(query)\n",
    "\n",
    "\n",
    "def search_bm25(query: str, bm25, df, top_k=5):\n",
    "    \"\"\"Search using BM25 and return top_k results.\"\"\"\n",
    "    print(f\"\\n🔍 Query: {query}\")\n",
    "    tokens = preprocess_query(query)\n",
    "    if not tokens:\n",
    "        print(\"⚠️ Query resulted in no valid tokens after preprocessing.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    scores = bm25.get_scores(tokens)\n",
    "    top_indices = scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "    print(f\"\\nTop {top_k} results:\")\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        title = df.loc[idx, \"title\"] if \"title\" in df.columns else \"(no title)\"\n",
    "        cal = df.loc[idx, \"calories_cal\"] if \"calories_cal\" in df.columns else \"?\"\n",
    "        print(f\"{rank}. {title}  ({cal} cal)\")\n",
    "\n",
    "    return df.iloc[top_indices][[\"title\", \"calories_cal\", \"totalfat_g\", \"protein_g\"]]\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Run example\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_dataset()\n",
    "    bm25, _ = build_bm25_index(df)\n",
    "\n",
    "    # Example queries\n",
    "    search_bm25(\"low fat chicken under 500 calories\", bm25, df, top_k=5)\n",
    "    search_bm25(\"high protein vegan salad\", bm25, df, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# semantic_retrieval.py (initial unoptimized version)\n",
    "# Task 4: Semantic Embeddings & Retrieval (BERT-based)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from text_preprocessing import preprocess_text\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Device check and model setup\n",
    "# -------------------------------------------------\n",
    "def setup_device_and_model(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Checks for GPU availability and loads the Sentence-BERT model accordingly.\n",
    "    Uses CUDA if available, otherwise falls back to CPU.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(f\"✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"⚠️ No GPU detected, using CPU\")\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    print(f\"Loaded model: {model_name} on {device.upper()}\")\n",
    "    return model, device\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Load dataset\n",
    "# -------------------------------------------------\n",
    "def load_dataset(input_path=\"data/hummus_recipes_preprocessed.csv\"):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"❌ Dataset not found at: {input_path}\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    print(f\"✅ Dataset loaded: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Build combined text field\n",
    "# -------------------------------------------------\n",
    "def build_combined_text(df):\n",
    "    \"\"\"\n",
    "    Combines relevant text columns into a single searchable document per recipe.\n",
    "    \"\"\"\n",
    "    text_cols = [\n",
    "        \"processed_title\",\n",
    "        \"processed_ingredients\",\n",
    "        \"processed_tags\",\n",
    "        \"processed_directions\"\n",
    "    ]\n",
    "    available_cols = [c for c in text_cols if c in df.columns]\n",
    "    print(f\"Using columns for embeddings: {available_cols}\")\n",
    "\n",
    "    df[\"combined_text\"] = df[available_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    print(\"✅ Combined text field created.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Create embeddings and save\n",
    "# -------------------------------------------------\n",
    "def create_and_save_embeddings(df, model, device, output_dir=\"data\", batch_size=96):\n",
    "    \"\"\"\n",
    "    Generates embeddings using Sentence-BERT and saves them as .npy.\n",
    "    Adjusts batch size automatically based on VRAM (~6GB default).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"recipe_embeddings.npy\")\n",
    "\n",
    "    print(f\"\\n⚙️  Generating embeddings on {device.upper()} (batch size={batch_size}) ...\")\n",
    "    texts = df[\"combined_text\"].tolist()\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    np.save(output_path, embeddings)\n",
    "    print(f\"✅ Embeddings created and saved → {output_path}\")\n",
    "    print(f\"Shape: {embeddings.shape}\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Semantic search\n",
    "# -------------------------------------------------\n",
    "def semantic_search(query, model, df, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform semantic retrieval based on cosine similarity between\n",
    "    the query embedding and recipe embeddings.\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Query: {query}\")\n",
    "    cleaned_query = \" \".join(preprocess_text(query))\n",
    "    query_emb = model.encode([cleaned_query], normalize_embeddings=True)\n",
    "\n",
    "    scores = cosine_similarity(query_emb, embeddings)[0]\n",
    "    top_indices = np.argsort(-scores)[:top_k]\n",
    "\n",
    "    results = df.iloc[top_indices][[\"title\", \"calories_cal\", \"totalfat_g\", \"protein_g\"]].copy()\n",
    "    results[\"similarity\"] = scores[top_indices]\n",
    "    print(\"\\nTop results:\\n\")\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Main\n",
    "# -------------------------------------------------\n",
    "def main():\n",
    "    model, device = setup_device_and_model()\n",
    "    df = load_dataset()\n",
    "    df = build_combined_text(df)\n",
    "\n",
    "    embeddings = create_and_save_embeddings(df, model, device, batch_size=96)\n",
    "\n",
    "    # Optional: Test queries\n",
    "    test_queries = [\n",
    "        \"low carb chicken meal\",\n",
    "        \"high protein vegan breakfast\",\n",
    "        \"sugar free dessert\",\n",
    "        \"quick healthy pasta dinner\"\n",
    "    ]\n",
    "\n",
    "    for q in test_queries:\n",
    "        semantic_search(q, model, df, embeddings, top_k=5)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
